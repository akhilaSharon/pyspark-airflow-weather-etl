PySpark + Airflow + Weather ETL (MinIO/S3)  
Build a real, production-style ETL: **Bronze â†’ Silver â†’ Gold** with PySpark, orchestrated by Airflow, stored in S3-compatible object storage (MinIO). Optional BI with Metabase.

<p align="left">
  <img alt="Python" src="https://img.shields.io/badge/Python-3.10+-blue">
  <img alt="PySpark" src="https://img.shields.io/badge/PySpark-3.x-orange">
  <img alt="Airflow" src="https://img.shields.io/badge/Airflow-2.x-%23017CEE">
  <img alt="Storage" src="https://img.shields.io/badge/Storage-MinIO%20(S3)-green">
  <img alt="License" src="https://img.shields.io/badge/License-MIT-lightgrey">
</p>

## ğŸ” What this project shows
- **Cloud-style storage locally:** S3 semantics via MinIO (swap to AWS/GCS/Azure later).
- **Data Lakehouse layers:** immutable **Bronze**, curated **Silver**, analytics-ready **Gold**.
- **Orchestration:** Airflow DAG with retries, idempotent writes, and parameterized dates.
- **PySpark data engineering:** partitioning, schema typing, small-file mitigation.
- **(Optional) BI:** push Gold to Postgres and visualize in Metabase.

## ğŸ“ Architecture
Open-Meteo API â†’ [Airflow: Extract] â†’ s3a://bronze/openmeteo/
â”‚
â””â”€ spark-submit â†’ s3a://silver/openmeteo/ (typed Parquet, partitioned by date)
â”‚
â””â”€ spark-submit â†’ s3a://gold/openmeteo/ (aggregates)
â””â”€(optional)â†’ Postgres â†’ Metabase


## ğŸ—‚ï¸ Repo structure
â”œâ”€ dags/ # Airflow DAGs
â”œâ”€ spark_jobs/ # PySpark jobs (silver, gold, optional loaders)
â”œâ”€ apps/ # Small extract scripts (requests â†’ JSON)
â”œâ”€ config/
â”‚ â””â”€ settings.yaml # Config: buckets, API params, etc.
â”œâ”€ docker/
â”‚ â”œâ”€ airflow/requirements.txt
â”‚ â””â”€ spark/spark-defaults.conf
â”œâ”€ tests/ # (coming) unit tests for transforms
â”œâ”€ README.md
â””â”€ .gitignore